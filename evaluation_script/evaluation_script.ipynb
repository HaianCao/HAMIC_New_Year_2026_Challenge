{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "280c8472",
   "metadata": {},
   "source": [
    "File này giúp thí sinh có cái nhìn tổng quan về cách tính điểm số trên leaderboard của BTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c41cad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "ROOT = os.path.join(os.getcwd(), 'sample_data')\n",
    "\n",
    "# ALGO Task\n",
    "ALGO_TASK_DIR = os.path.join(ROOT, 'ALGO_Task')\n",
    "ALGO_DATASET = os.path.join(ALGO_TASK_DIR, 'dataset.txt')\n",
    "ALGO_PRED = os.path.join(ALGO_TASK_DIR, 'pred.csv')\n",
    "\n",
    "# CV Task\n",
    "CV_TASK_DIR = os.path.join(ROOT, 'CV_Task')\n",
    "CV_GOLD = os.path.join(CV_TASK_DIR, 'gold.csv')\n",
    "CV_PRED = os.path.join(CV_TASK_DIR, 'pred.csv')\n",
    "CV_TEST = os.path.join(CV_TASK_DIR, 'test.csv')\n",
    "\n",
    "# NLP Task\n",
    "NLP_TASK_DIR = os.path.join(ROOT, 'NLP_Task')\n",
    "NLP_GOLD = os.path.join(NLP_TASK_DIR, 'gold.csv')\n",
    "NLP_PRED = os.path.join(NLP_TASK_DIR, 'pred.csv')\n",
    "NLP_TEST = os.path.join(NLP_TASK_DIR, 'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6ea8a2",
   "metadata": {},
   "source": [
    "# CV Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fadecf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cv_task_score(gold_file, pred_file):\n",
    "    # Đọc dữ liệu từ file CSV\n",
    "    gold_df = pd.read_csv(gold_file)\n",
    "    pred_df = pd.read_csv(pred_file)\n",
    "    \n",
    "    # Merge hai dataframe dựa trên cột ID để đảm bảo khớp dữ liệu\n",
    "    merged_df = pd.merge(gold_df, pred_df, on='ID', suffixes=('_gold', '_pred'))\n",
    "    \n",
    "    y_true = merged_df['Age_gold']\n",
    "    y_pred = merged_df['Age_pred']\n",
    "    \n",
    "    # Tính RMSE\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de34325a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Điểm RMSE: 2.0\n"
     ]
    }
   ],
   "source": [
    "cv_score = cv_task_score(CV_GOLD, CV_PRED)\n",
    "print(f\"Điểm RMSE: {cv_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e3a172",
   "metadata": {},
   "source": [
    "# NLP Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf308451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_task_score(gold_file, pred_file):\n",
    "    # Đọc file, ép kiểu ID thành string\n",
    "    gold_df = pd.read_csv(gold_file, dtype={'ID': str})\n",
    "    pred_df = pd.read_csv(pred_file, dtype={'ID': str})\n",
    "    \n",
    "    # Merge để so khớp ID\n",
    "    merged = pd.merge(gold_df, pred_df, on='ID', suffixes=('_gold', '_pred'))\n",
    "    \n",
    "    # Tính Macro F1\n",
    "    valid_labels = [\"anger\", \"disgust\", \"enjoyment\", \"fear\", \"sadness\", \"surprise\", \"other\"]\n",
    "    \n",
    "    score = f1_score(\n",
    "        y_true=merged['Emotion_gold'], \n",
    "        y_pred=merged['Emotion_pred'], \n",
    "        labels=valid_labels, \n",
    "        average='macro',\n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63d9ef4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP Score: 0.20952\n"
     ]
    }
   ],
   "source": [
    "nlp_score = nlp_task_score(NLP_GOLD, NLP_PRED)\n",
    "print(f\"NLP Score: {nlp_score:.5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf9052f",
   "metadata": {},
   "source": [
    "# ALGO Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63ebbb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_id(val: Any) -> str:\n",
    "    v_str = str(val).strip()\n",
    "    try:\n",
    "        return str(int(float(v_str)))\n",
    "    except ValueError:\n",
    "        return v_str\n",
    "\n",
    "class ProblemInstance:\n",
    "    def __init__(self, test_id: int):\n",
    "        self.test_id = test_id\n",
    "        self.vehicles = {}\n",
    "        self.orders = {}\n",
    "        self.min_vehicle_price = float('inf')\n",
    "    def add_vehicle(self, v_id, capacity, price, start_x, start_y):\n",
    "        self.vehicles[normalize_id(v_id)] = {\n",
    "            \"capacity\": capacity, \"price\": price, \"start\": (start_x, start_y)\n",
    "        }\n",
    "        if price < self.min_vehicle_price: self.min_vehicle_price = price\n",
    "    def add_order(self, o_id, weight, p_x, p_y, d_x, d_y):\n",
    "        self.orders[normalize_id(o_id)] = {\n",
    "            \"weight\": weight, \"pickup\": (p_x, p_y), \"delivery\": (d_x, d_y)\n",
    "        }\n",
    "    def get_lower_bound(self) -> float:\n",
    "        if not self.vehicles or not self.orders: return 0.0\n",
    "        dist = sum(math.sqrt((o[\"pickup\"][0]-o[\"delivery\"][0])**2 + (o[\"pickup\"][1]-o[\"delivery\"][1])**2) \n",
    "                   for o in self.orders.values())\n",
    "        return dist * self.min_vehicle_price\n",
    "\n",
    "class InputParser:\n",
    "    @staticmethod\n",
    "    def parse(filepath: str) -> Dict[int, ProblemInstance]:\n",
    "        if not os.path.exists(filepath): raise FileNotFoundError(f\"Dataset not found: {filepath}\")\n",
    "        tests = {}\n",
    "        current_test = None\n",
    "        section = None\n",
    "        with open(filepath, \"r\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line: continue\n",
    "                if line.startswith(\"Test \"):\n",
    "                    try: \n",
    "                        t_id = int(line.split(\" \")[1])\n",
    "                        current_test = ProblemInstance(t_id)\n",
    "                        tests[t_id] = current_test\n",
    "                        section = None\n",
    "                    except: pass\n",
    "                    continue\n",
    "                if line == \"Vehicle\": section = \"Vehicle\"; continue\n",
    "                if line == \"Order\": section = \"Order\"; continue\n",
    "                if line.startswith(\"ID,\"): continue\n",
    "                \n",
    "                if current_test:\n",
    "                    p = line.split(\",\")\n",
    "                    if section == \"Vehicle\" and len(p)>=5:\n",
    "                        current_test.add_vehicle(p[0], float(p[1]), float(p[2]), float(p[3]), float(p[4]))\n",
    "                    elif section == \"Order\" and len(p)>=6:\n",
    "                        current_test.add_order(p[0], float(p[1]), float(p[2]), float(p[3]), float(p[4]), float(p[5]))\n",
    "        return tests\n",
    "\n",
    "class RouteValidator:\n",
    "    @staticmethod\n",
    "    def dist(p1, p2): return math.sqrt((p1[0]-p2[0])**2 + (p1[1]-p2[1])**2)\n",
    "    @staticmethod\n",
    "    def evaluate(instance, sub_df):\n",
    "        df = sub_df[sub_df['Test'] == instance.test_id].copy()\n",
    "        if df.empty: return float('inf')\n",
    "        \n",
    "        # Sort by Vehicle -> Stop_Order\n",
    "        df['Stop_Order'] = pd.to_numeric(df['Stop_Order'], errors='coerce')\n",
    "        if df['Stop_Order'].isnull().any(): return float('inf')\n",
    "        df = df.sort_values(['Vehicle_ID', 'Stop_Order'])\n",
    "        \n",
    "        states = {oid: \"PENDING\" for oid in instance.orders}\n",
    "        v_owner = {}\n",
    "        total_cost = 0.0\n",
    "        for v_id, route in df.groupby('Vehicle_ID'):\n",
    "            if v_id not in instance.vehicles: return float('inf')\n",
    "            v_data = instance.vehicles[v_id]\n",
    "            curr_load, curr_pos = 0.0, v_data['start']\n",
    "            \n",
    "            for _, row in route.iterrows():\n",
    "                o_id, act = row['Order_ID'], str(row['Type']).strip()\n",
    "                if o_id not in instance.orders: return float('inf')\n",
    "                o_data = instance.orders[o_id]\n",
    "                \n",
    "                target = None\n",
    "                if act == 'Pickup':\n",
    "                    if states[o_id] != \"PENDING\": return float('inf')\n",
    "                    target, curr_load = o_data['pickup'], curr_load + o_data['weight']\n",
    "                    states[o_id], v_owner[o_id] = \"PICKED\", v_id\n",
    "                elif act == 'Delivery':\n",
    "                    if states[o_id] != \"PICKED\" or v_owner.get(o_id) != v_id: return float('inf')\n",
    "                    target, curr_load = o_data['delivery'], curr_load - o_data['weight']\n",
    "                    states[o_id] = \"DELIVERED\"\n",
    "                else: return float('inf')\n",
    "                if curr_load > v_data['capacity'] + 1e-9: return float('inf')\n",
    "                total_cost += RouteValidator.dist(curr_pos, target) * v_data['price']\n",
    "                curr_pos = target\n",
    "        if any(s != \"DELIVERED\" for s in states.values()): return float('inf')\n",
    "        return total_cost\n",
    "        \n",
    "def algo_task_score(dataset_path, pred_file):\n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(f\"[ALGO-Error] Dataset not found: {dataset_path}\")\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Load Data\n",
    "        data = InputParser.parse(dataset_path)\n",
    "        submission = pd.read_csv(pred_file)\n",
    "        \n",
    "        # Preprocessing\n",
    "        if 'Test' in submission.columns: \n",
    "            submission['Test'] = pd.to_numeric(submission['Test'], errors='coerce').fillna(-1).astype(int)\n",
    "        for col in ['Vehicle_ID', 'Order_ID']:\n",
    "            if col in submission.columns: submission[col] = submission[col].apply(normalize_id)\n",
    "        target_tests = set(data.keys()) # Score all tests in dataset\n",
    "        total_eff, valid_count = 0.0, 0\n",
    "        \n",
    "        for t_id in target_tests:\n",
    "            if t_id not in data: continue\n",
    "            instance = data[t_id]\n",
    "            lb = instance.get_lower_bound()\n",
    "            if lb <= 1e-9: continue\n",
    "            \n",
    "            cost = RouteValidator.evaluate(instance, submission)\n",
    "            if cost != float('inf'):\n",
    "                total_eff += (cost / lb)\n",
    "                valid_count += 1\n",
    "                \n",
    "        return total_eff / valid_count if valid_count > 0 else float('inf')\n",
    "    except Exception as e:\n",
    "        print(f\"[ALGO-Error] {str(e)}\")\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0cf3b105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALGO Score: 0.93823\n"
     ]
    }
   ],
   "source": [
    "algo_score = algo_task_score(ALGO_DATASET, ALGO_PRED)\n",
    "print(f\"ALGO Score: {algo_score:.5}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
